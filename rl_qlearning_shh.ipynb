{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38993522",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import py4j\n",
    "from py4j.java_gateway import JavaGateway\n",
    "import random\n",
    "import time\n",
    "from gym.spaces import Discrete, Box, Dict, Tuple, MultiBinary, MultiDiscrete\n",
    "import numpy as np\n",
    "import math\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "from time import process_time\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5817ec48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HHEnv(gym.Env):\n",
    "    def __init__(self,problem_domain = 'TSP', time_limit=600,lubylength = 200000):\n",
    "        hyflex = JavaGateway().jvm\n",
    "        self.action_space = None\n",
    "        self.problem_domain = problem_domain\n",
    "        if problem_domain == 'SAT':   \n",
    "            self.problem = hyflex.SAT.SAT(1234)\n",
    "        elif problem_domain =='BP':\n",
    "            self.problem = hyflex.BinPacking.BinPacking(1234) \n",
    "        elif problem_domain == 'TSP':\n",
    "            self.problem = hyflex.travelingSalesmanProblem.TSP(1234)\n",
    "        \n",
    "        self.last_h = 9\n",
    "        self.tail_length = 10\n",
    "        \n",
    "        low = np.array(\n",
    "            [   -1,\n",
    "                -1\n",
    "            ],\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "        high = np.array(\n",
    "            [   self.last_h,\n",
    "                self.tail_length\n",
    "            ],\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "        self.observation_space = Box(low, high)\n",
    "        self.state = None\n",
    "        self.done = False\n",
    "        self.solution = None\n",
    "        self.h_chain = None\n",
    "        self.luby = None\n",
    "        self.episode = 0\n",
    "        self.lubylength = lubylength\n",
    "        self.origin_sol = None\n",
    "        self.time_limit = time_limit\n",
    "        self.current_time = None\n",
    "        self.overall_time = 0\n",
    "        self.exceed = False\n",
    "        self.totalDelta = 0\n",
    "        self.sidewaysMoveFound = False\n",
    "        self.costCurrent = None\n",
    "        \n",
    "        def luby(length = 200000):\n",
    "            L = [-1]*length\n",
    "            for i in range (1,length+1):\n",
    "        #         print(\"current index: \",i)\n",
    "                left = math.log2(i + 1)\n",
    "                right = int(left)\n",
    "                k = right\n",
    "                if left == right:\n",
    "                    L[i-1] = 2 **(k-1)\n",
    "                else:\n",
    "                    k+=1\n",
    "                    L[i-1] = L[i-1 - 2**(k-1) + 1]\n",
    "            return L\n",
    "        \n",
    "        self.luby = luby(length=self.lubylength)\n",
    "        \n",
    "    def step(self, action):\n",
    "        \n",
    "        # setting time \n",
    "        t1 = time.time()\n",
    "        \n",
    "        # last heuristic applied\n",
    "        last_heu = self.h_chain[self.iter_num-1]\n",
    "        \n",
    "        # update hyper-heuristic chain\n",
    "        self.h_chain[self.iter_num] = int(action)\n",
    "        \n",
    "        self.iter_num += 1  \n",
    "        \n",
    "        # number of heuristic left if bigger than max tail length it equal max tail length\n",
    "        num_heur_left = len(self.h_chain) - self.iter_num\n",
    "        \n",
    "        num_heur_left = min(self.tail_length, num_heur_left)\n",
    "        \n",
    "        ori = self.origin_sol\n",
    "        \n",
    "        self.origin_sol = self.problem.applyHeuristic(int(action), 1, 1)\n",
    "        delta = self.origin_sol - ori\n",
    "        \n",
    "        t2 = time.time()\n",
    "    \n",
    "        self.current_time += t2-t1\n",
    "        self.overall_time += t2-t1\n",
    "        self.totalDelta+=delta\n",
    "\n",
    "        end = True if (len(self.h_chain)) == self.iter_num else False\n",
    "        \n",
    "        if end:\n",
    "            self.done = True\n",
    "\n",
    "        if self.totalDelta <0:\n",
    "            self.current_time = max(1,self.current_time)\n",
    "            reward = (- self.totalDelta)/(self.current_time)\n",
    "        else:\n",
    "            reward = 0\n",
    "    \n",
    "            \n",
    "        self.state = (last_heu, num_heur_left)\n",
    "        \n",
    "        info = {}\n",
    "        # Return step information\n",
    "        self.solution = self.problem.getBestSolutionValue()\n",
    "        if self.overall_time > self.time_limit:\n",
    "            self.exceed == True\n",
    "        if self.totalDelta < 0:\n",
    "            self.problem.copySolution(1,0)\n",
    "            self.done = True\n",
    "        elif self.totalDelta == 0:\n",
    "            self.problem.copySolution(1,2)\n",
    "            self.sidewaysMoveFound = True\n",
    "        elif end and self.sidewaysMoveFound and self.totalDelta >= 0:\n",
    "            self.problem.copySolution(2,0)\n",
    "            \n",
    "        return np.array(self.state, dtype=np.float32), reward, self.done, info\n",
    "\n",
    "    def render(self):\n",
    "        # Implement viz\n",
    "        pass\n",
    "        \n",
    "    \n",
    "    def initialize(self,instance):\n",
    "        self.problem.loadInstance(instance)\n",
    "        self.problem.setMemorySize(4)\n",
    "        self.problem.initialiseSolution(0)\n",
    "        self.problem.copySolution(0,1)\n",
    "        self.problem.copySolution(0,3)\n",
    "        self.origin_sol = self.problem.getFunctionValue(0)\n",
    "        self.action_space = Discrete(self.problem.getNumberOfHeuristics())\n",
    "        self.iter_num = 0\n",
    "        self.current_time = 0\n",
    "        self.h_chain = [-1 for i in range(self.luby[self.episode])]\n",
    "        self.episode += 1\n",
    "        self.state = (-1, min(10,len(self.h_chain)))\n",
    "        self.sidewaysMoveFound = False\n",
    "        self.done = False\n",
    "        return np.array(self.state, dtype=np.float32)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.origin_sol = self.problem.getBestSolutionValue()\n",
    "        self.problem.copySolution(0,1)\n",
    "        self.sidewaysMoveFound = False\n",
    "        self.iter_num = 0\n",
    "        self.current_time = 0\n",
    "        self.totalDelta = 0\n",
    "        self.h_chain = [-1 for i in range(self.luby[self.episode])]\n",
    "        self.episode += 1\n",
    "        self.state = (-1, min(10,len(self.h_chain)))\n",
    "        self.done = False\n",
    "        return np.array(self.state, dtype=np.float32)\n",
    "    \n",
    "    def start_from_previous(self):\n",
    "        self.problem.copySolution(3, 1)\n",
    "    \n",
    "    def store_best(self):\n",
    "        self.problem.copySolution(0, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ce6547",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HHagent():\n",
    "    def __init__(self, problem_domain = 'SAT',\n",
    "                 instance = 3, time = 100, min_lr = None, \n",
    "                 min_epsilon = None, discount = None, decay = None,  unimprovement_time = None,\n",
    "                 is_backtrack = False, random_store = False, is_restart = False, res_q = False):\n",
    "        \n",
    "        self.problem_domain = problem_domain\n",
    "        self.env = HHEnv(self.problem_domain)\n",
    "        self.instance = instance\n",
    "        self.env.initialize(self.instance)        \n",
    "        self.buckets = (self.env.action_space.n + 1, self.env.tail_length + 1)\n",
    "        \n",
    "        self.time = time\n",
    "        self.min_lr = 0.3\n",
    "        self.min_epsilon = 0.1\n",
    "        self.discount = 0.5\n",
    "        self.decay = 25\n",
    "        self.unimprovement_time = 20\n",
    "        self.is_backtrack = is_backtrack\n",
    "        self.random_store = random_store\n",
    "        self.is_restart = is_restart\n",
    "        self.res_q = res_q\n",
    "        self.Q_table = np.zeros(self.buckets + (self.env.action_space.n,))\n",
    "        self.funV = []\n",
    "        self.functionValue = []\n",
    "        self.Bests = []\n",
    "        \n",
    "        \n",
    "    def discretize_state(self, obs):\n",
    "        discretized = list()\n",
    "        for i in range(len(obs)):\n",
    "            discretized.append(int(obs[i]))\n",
    "\n",
    "        return tuple(discretized)\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if (np.random.random() < self.epsilon):\n",
    "            return self.env.action_space.sample() \n",
    "        else:\n",
    "            return np.argmax(self.Q_table[state])\n",
    "\n",
    "    def update_q(self, state, action, reward, new_state):\n",
    "        self.Q_table[state][action] += (self.learning_rate * \n",
    "                                        (reward \n",
    "                                         + self.discount * np.max(self.Q_table[new_state]) \n",
    "                                         - self.Q_table[state][action]))\n",
    "        \n",
    "\n",
    "    def get_epsilon(self, t):\n",
    "        \n",
    "        return max(self.min_epsilon, min(1., 1. - math.log10((t + 1) / self.decay)))\n",
    "\n",
    "    def get_learning_rate(self, t):\n",
    "        return max(self.min_lr, min(1., 1. - math.log10((t + 1) / self.decay)))\n",
    "    \n",
    "    \n",
    "    def train(self):\n",
    "        e = -1\n",
    "        overall_time = 0\n",
    "        start_time = time.time()\n",
    "        best = np.inf\n",
    "        best_time = start_time\n",
    "        best_of_all_time = np.inf\n",
    "        \n",
    "        while self.time > overall_time:\n",
    "            e += 1\n",
    "            end_time = time.time()\n",
    "            overall_time = end_time - start_time\n",
    "            \n",
    "            # reset or intiailize the state\n",
    "            if end_time - best_time > self.unimprovement_time:\n",
    "                if self.is_restart:\n",
    "                    print(\"+++++++++UPDATE_RESTART+++++++++++++++++\")\n",
    "                    self.Bests.append(self.env.problem.getBestSolutionValue())\n",
    "                    best = np.inf\n",
    "                    self.env = HHEnv(self.problem_domain)\n",
    "                    current_state = self.discretize_state(self.env.initialize(self.instance))\n",
    "                    if self.res_q:\n",
    "                        self.Q_table = np.zeros(self.buckets + (self.env.action_space.n,))\n",
    "                if self.is_backtrack:\n",
    "                    print(\"+++++++++Go to Random Best+++++++++++++++++\")\n",
    "                    self.Bests.append(self.env.problem.getBestSolutionValue())\n",
    "                    self.env.start_from_previous()\n",
    "                    best = np.inf\n",
    "                    current_state = self.discretize_state(self.env.reset())\n",
    "                else:\n",
    "                    self.Bests.append(self.env.problem.getBestSolutionValue())\n",
    "                    current_state = self.discretize_state(self.env.reset())\n",
    "            else:\n",
    "                self.Bests.append(self.env.problem.getBestSolutionValue())\n",
    "                current_state = self.discretize_state(self.env.reset())\n",
    "\n",
    "            self.learning_rate = self.get_learning_rate(e)\n",
    "            self.epsilon = self.get_epsilon(e)\n",
    "            done = False\n",
    "            \n",
    "            # Looping for each step\n",
    "            while not done:\n",
    "                # Choose A from S\n",
    "                action = self.choose_action(current_state)\n",
    "                \n",
    "                # Take action\n",
    "                obs, reward, done, _ = self.env.step(action)\n",
    "                new_state = self.discretize_state(obs)\n",
    "                # Update Q(S,A)\n",
    "                self.update_q(current_state, action, reward, new_state)\n",
    "                current_state = new_state\n",
    "                \n",
    "                # We break out of the loop when done is False which is\n",
    "                # a terminal state.\n",
    "            if e % 500 == 0:\n",
    "                print(\"Current Funciton Value: {}\".format(self.env.problem.getFunctionValue(0)))\n",
    "            \n",
    "            if self.env.problem.getFunctionValue(0) < best:\n",
    "    \n",
    "                best = self.env.problem.getFunctionValue(0)\n",
    "                best_time = end_time\n",
    "                if self.is_backtrack:\n",
    "                    if self.random_store:\n",
    "                        random_store = random.uniform(0, 1)\n",
    "                        if random_store < 0.3:\n",
    "                            self.env.store_best()\n",
    "                    self.env.store_best()\n",
    "            \n",
    "            if best_of_all_time > self.env.problem.getBestSolutionValue():\n",
    "                best_of_all_time = self.env.problem.getBestSolutionValue()\n",
    "                \n",
    "            self.funV.append(self.env.problem.getFunctionValue(0))\n",
    "        print('Finished training! Episode is {}'.format(e))\n",
    "        print(\"Recorded best solutnion is {}\".format(best_of_all_time))\n",
    "    \n",
    "        return self.env.problem.getBestSolutionValue(), min(self.Bests)\n",
    "    \n",
    "    def plot_learning(self):\n",
    "        \"\"\"\n",
    "        Plots the number of steps at each episode and prints the\n",
    "        amount of times that an episode was successfully completed.\n",
    "        \"\"\"\n",
    "        sns.lineplot(x = range(len(self.funV)), y = self.funV)\n",
    "        \n",
    "    def save(self):\n",
    "        np.save('Qtable',self.Q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd0a14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_q_learning():\n",
    "     agent = HHagent(problem_domain='SAT',instance = 3,time = 50)\n",
    "     a, b = agent.train()\n",
    "     agent.plot_learning()\n",
    "     \n",
    "     return a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee95de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all():\n",
    "    best_org = []\n",
    "    best_restart = []\n",
    "    best_restart_q = []\n",
    "    best_backtrack = []\n",
    "    best_backtrack_random = []\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.title('org')\n",
    "    for i in range(2):\n",
    "#         print(f'######{i}for org')\n",
    "        agent = HHagent(problem_domain='SAT',instance = 3,time = 600)\n",
    "        print(\"Training orginal\")\n",
    "        best = agent.train()\n",
    "        best_org.append(best)\n",
    "        agent.plot_learning()\n",
    "\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.title('Restart')\n",
    "    for i in range(2):\n",
    "#         print(f'######{i}for restart')\n",
    "        agent = HHagent(problem_domain = 'SAT', instance = 3, time = 600, is_restart = True)\n",
    "        print('Training Restart')\n",
    "        best = agent.train()\n",
    "        best_restart.append(best)\n",
    "        agent.plot_learning()\n",
    "        \n",
    "    plt.figure()\n",
    "    plt.title('Restart with q')\n",
    "    for i in range(2):\n",
    "#         print(f'######{i}for Restart with q')\n",
    "        agent = HHagent(problem_domain = 'SAT', instance = 3, time = 600, is_restart = True, res_q = True)\n",
    "        print('Training Restart and Q')\n",
    "        best = agent.train()\n",
    "        best_restart_q.append(best)\n",
    "        agent.plot_learning()\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.title('Backtrack')\n",
    "    for i in range(2):\n",
    "        (f'######{i}for Restart with q')\n",
    "        agent = HHagent(problem_domain = 'SAT', instance = 3, time = 600, is_backtrack = True)\n",
    "        print(\"Training Backtrack\")\n",
    "        best = agent.train()\n",
    "        best_backtrack.append(best)\n",
    "        agent.plot_learning() \n",
    "    \n",
    "    plt.figure()\n",
    "    plt.title('Backtrack with random')\n",
    "    for i in range(2):\n",
    "        agent = HHagent(problem_domain = 'SAT', instance = 3, time = 600, is_backtrack = True, random_store = True)\n",
    "        print(\"Training BackTrack with Random\")\n",
    "        best = agent.train()\n",
    "        best_backtrack_random.append(best)\n",
    "        agent.plot_learning() \n",
    "    \n",
    "    \n",
    "    return best_org, best_restart, best_restart_q, best_backtrack, best_backtrack_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f8b6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_org, best_restart, best_restart_q, best_backtrack, best_backtrack_random = run_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6ff07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"best_org\", best_org)\n",
    "print(np.mean(best_org))\n",
    "print(\"best_restart\", best_restart)\n",
    "print(np.mean(best_restart))\n",
    "print(\"best_restart_q\", best_restart_q)\n",
    "print(np.mean(best_restart_q))\n",
    "print(\"best_backtrack\", best_backtrack)\n",
    "print(np.mean(best_backtrack))\n",
    "print(\"best_backtrack_random\", best_backtrack_random)\n",
    "print(np.mean(best_backtrack_random))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e3f6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all():\n",
    "    best_org = []\n",
    "    best_restart = []\n",
    "    best_restart_q = []\n",
    "    best_backtrack = []\n",
    "    best_backtrack_random = []\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.title('org')\n",
    "    for i in range(4):\n",
    "#         print(f'######{i}for org')\n",
    "        agent = HHagent(problem_domain='TSP',instance = 0,time = 600)\n",
    "        print(\"Training orginal\")\n",
    "        best = agent.train()\n",
    "        best_org.append(best)\n",
    "        agent.plot_learning()\n",
    "\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.title('Restart')\n",
    "    for i in range(4):\n",
    "#         print(f'######{i}for restart')\n",
    "        agent = HHagent(problem_domain = 'TSP', instance = 0, time = 600, is_restart = True)\n",
    "        print('Training Restart')\n",
    "        best = agent.train()\n",
    "        best_restart.append(best)\n",
    "        agent.plot_learning()\n",
    "        \n",
    "    plt.figure()\n",
    "    plt.title('Restart with q')\n",
    "    for i in range(4):\n",
    "#         print(f'######{i}for Restart with q')\n",
    "        agent = HHagent(problem_domain = 'TSP', instance = 0, time = 600, is_restart = True, res_q = True)\n",
    "        print('Training Restart and Q')\n",
    "        best = agent.train()\n",
    "        best_restart_q.append(best)\n",
    "        agent.plot_learning()\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.title('Backtrack')\n",
    "    for i in range(4):\n",
    "        (f'######{i}for Restart with q')\n",
    "        agent = HHagent(problem_domain = 'TSP', instance = 0, time = 600, is_backtrack = True)\n",
    "        print(\"Training Backtrack\")\n",
    "        best = agent.train()\n",
    "        best_backtrack.append(best)\n",
    "        agent.plot_learning() \n",
    "    \n",
    "    plt.figure()\n",
    "    plt.title('Backtrack with random')\n",
    "    for i in range(4):\n",
    "        agent = HHagent(problem_domain = 'TSP', instance = 0, time = 600, is_backtrack = True, random_store = True)\n",
    "        print(\"Training BackTrack with Random\")\n",
    "        best = agent.train()\n",
    "        best_backtrack_random.append(best)\n",
    "        agent.plot_learning() \n",
    "    \n",
    "    \n",
    "    return best_org, best_restart, best_restart_q, best_backtrack, best_backtrack_random\n",
    "\n",
    "\n",
    "best_org, best_restart, best_restart_q, best_backtrack, best_backtrack_random = run_all()\n",
    "print(\"best_org\", best_org)\n",
    "print(np.mean(best_org))\n",
    "print(\"best_restart\", best_restart)\n",
    "print(np.mean(best_restart))\n",
    "print(\"best_restart_q\", best_restart_q)\n",
    "print(np.mean(best_restart_q))\n",
    "print(\"best_backtrack\", best_backtrack)\n",
    "print(np.mean(best_backtrack))\n",
    "print(\"best_backtrack_random\", best_backtrack_random)\n",
    "print(np.mean(best_backtrack_random))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78624735",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all():\n",
    "    best_org = []\n",
    "    best_restart = []\n",
    "    best_restart_q = []\n",
    "    best_backtrack = []\n",
    "    best_backtrack_random = []\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.title('org')\n",
    "    for i in range(2):\n",
    "#         print(f'######{i}for org')\n",
    "        agent = HHagent(problem_domain='TSP',instance = 3,time = 600)\n",
    "        print(\"Training orginal\")\n",
    "        best = agent.train()\n",
    "        best_org.append(best)\n",
    "        agent.plot_learning()\n",
    "\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.title('Restart')\n",
    "    for i in range(2):\n",
    "#         print(f'######{i}for restart')\n",
    "        agent = HHagent(problem_domain = 'TSP', instance = 3, time = 600, is_restart = True)\n",
    "        print('Training Restart')\n",
    "        best = agent.train()\n",
    "        best_restart.append(best)\n",
    "        agent.plot_learning()\n",
    "        \n",
    "    plt.figure()\n",
    "    plt.title('Restart with q')\n",
    "    for i in range(2):\n",
    "#         print(f'######{i}for Restart with q')\n",
    "        agent = HHagent(problem_domain = 'TSP', instance = 3, time = 600, is_restart = True, res_q = True)\n",
    "        print('Training Restart and Q')\n",
    "        best = agent.train()\n",
    "        best_restart_q.append(best)\n",
    "        agent.plot_learning()\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.title('Backtrack')\n",
    "    for i in range(2):\n",
    "        (f'######{i}for Restart with q')\n",
    "        agent = HHagent(problem_domain = 'TSP', instance = 3, time = 600, is_backtrack = True)\n",
    "        print(\"Training Backtrack\")\n",
    "        best = agent.train()\n",
    "        best_backtrack.append(best)\n",
    "        agent.plot_learning() \n",
    "    \n",
    "    plt.figure()\n",
    "    plt.title('Backtrack with random')\n",
    "    for i in range(2):\n",
    "        agent = HHagent(problem_domain = 'TSP', instance = 3, time = 600, is_backtrack = True, random_store = True)\n",
    "        print(\"Training BackTrack with Random\")\n",
    "        best = agent.train()\n",
    "        best_backtrack_random.append(best)\n",
    "        agent.plot_learning() \n",
    "    \n",
    "    \n",
    "    return best_org, best_restart, best_restart_q, best_backtrack, best_backtrack_random\n",
    "\n",
    "best_org, best_restart, best_restart_q, best_backtrack, best_backtrack_random = run_all()\n",
    "print(\"best_org\", best_org)\n",
    "print(np.mean(best_org))\n",
    "print(\"best_restart\", best_restart)\n",
    "print(np.mean(best_restart))\n",
    "print(\"best_restart_q\", best_restart_q)\n",
    "print(np.mean(best_restart_q))\n",
    "print(\"best_backtrack\", best_backtrack)\n",
    "print(np.mean(best_backtrack))\n",
    "print(\"best_backtrack_random\", best_backtrack_random)\n",
    "print(np.mean(best_backtrack_random))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea81ceb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:gym38]",
   "language": "python",
   "name": "conda-env-gym38-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "c10cbfd87b8d929fea9e203bdf98fbb335344aa3e0793f6a667d786b98e27b70"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
